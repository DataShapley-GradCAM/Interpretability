{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cld2_r0c9D44"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils import data\n",
        "#import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import os\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import csv\n",
        "import zipfile\n",
        "from torch.utils.data import Dataset\n",
        "from natsort import natsorted\n",
        "from PIL import Image\n",
        "import sklearn\n",
        "%matplotlib inline\n",
        "import gdown"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yel8lCiY9Ipx",
        "outputId": "cdb2176c-369e-423c-edc9-65124ceaa543"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('drive/My Drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsbgLF0l9MRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "655d3da9-e8dd-44fa-d73f-12ac3a4e6c6d"
      },
      "source": [
        "manual_seed = 999\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0a95297910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2He8ObGUxeKe"
      },
      "source": [
        "#                              Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieKzfVoz9Xws",
        "outputId": "4b83f7a5-8f5d-4a4a-d96a-1a87a4a94b5f"
      },
      "source": [
        "# Root directory for the celebset dataset\n",
        "data_root = 'data/celebset'\n",
        "# Path to folder with individual images\n",
        "img_folder1 = f'{data_root}/img_celebset'\n",
        "# URL for the Celebset dataset\n",
        "url = \"https://drive.google.com/uc?id=1EGVVqy9ZUVg4ErPlgEGQuezDkcMPJBSh\"\n",
        "# Path to download the dataset to\n",
        "download_path = f'{data_root}/img_celebset.zip'\n",
        "# Create required directories \n",
        "if not os.path.exists(data_root):\n",
        "  os.makedirs(data_root)\n",
        "  os.makedirs(img_folder1)\n",
        "\n",
        "# Download the dataset from google drive\n",
        "gdown.download(url, download_path, quiet=False)\n",
        "\n",
        "# Unzip the downloaded file \n",
        "with zipfile.ZipFile(download_path, 'r') as ziphandler:\n",
        "  ziphandler.extractall(img_folder1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EGVVqy9ZUVg4ErPlgEGQuezDkcMPJBSh\n",
            "To: /content/data/celebset/img_celebset.zip\n",
            "11.0MB [00:00, 20.2MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amd7OgfR9cT0"
      },
      "source": [
        "class CelebADataset(Dataset):\n",
        "  def __init__(self, root_dir, transform=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      root_dir (string): Directory with all the images\n",
        "      transform (callable, optional): transform to be applied to each image sample\n",
        "    \"\"\"\n",
        "    # Read names of images in the root directory\n",
        "    image_names = os.listdir(root_dir)\n",
        "\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform \n",
        "    self.image_names = natsorted(image_names)\n",
        "\n",
        "  def __len__(self): \n",
        "    return len(self.image_names)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Get the path to the image \n",
        "    img_path = os.path.join(self.root_dir, self.image_names[idx])\n",
        "    # Load image and convert it to RGB\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    # Apply transformations to the image\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    return img"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2Bjm7ez9faK"
      },
      "source": [
        "# Spatial size of training images, images are resized to this size.\n",
        "image_size = 32\n",
        "# Transformations to be applied to each individual image sample\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                          std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "inv_normalize =  transforms.Normalize(\n",
        "    mean=[-1, -1, -1],\n",
        "    std=[1/0.5, 1/0.5, 1/0.5]\n",
        ")\n",
        "# Load the dataset from file and apply transformations\n",
        "celebset_dataset= CelebADataset(f'{img_folder1}/img_celebset', transform)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv3xZNOE9mDQ"
      },
      "source": [
        "attributes1 = pd.read_csv('/content/drive/My Drive/list_attr_celebset.csv')\n",
        "y1=attributes1['male']\n",
        "\n",
        "dataset_X=[]\n",
        "dataset_y=[]\n",
        "for i in range(1600):\n",
        "  #if i not in test_inddex:\n",
        "    dataset_X.append(celebset_dataset.__getitem__(i))\n",
        "    dataset_y.append(y1[i])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjo9nCxlwnxJ"
      },
      "source": [
        "encoder={1:\"male\",0:\"female\"}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiai0eMP96aB"
      },
      "source": [
        "#obtain the trainset of the fold that corresponds to the best accuracy\n",
        "df = pd.read_excel ('/content/drive/MyDrive/test_train_valid.xlsx')\n",
        "train_idx=df.iloc[7]\n",
        "train_idx=np.array(train_idx)[1:]\n",
        "train_x = []\n",
        "train_y=[]\n",
        "for i in range(len(train_idx)):\n",
        "    train_x.append(dataset_X[int(train_idx[i])])\n",
        "    train_y.append(dataset_y[int(train_idx[i])])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igts90hp0-Jv"
      },
      "source": [
        "#              **Get indices and values of data points**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xuPAxEqBGOp"
      },
      "source": [
        "indices_df= pd.read_csv(\"/content/drive/MyDrive/ShapleyIndicesandValuesFinal.csv\",header=None)\n",
        "low_indices=list(indices_df.iloc[:52,0])\n",
        "high_indices=list(indices_df.iloc[52:,0])\n",
        "high_indices.reverse()\n",
        "trainx_low=[]\n",
        "trainy_low=[]\n",
        "trainx_high=[]\n",
        "trainy_high=[]\n",
        "for i in low_indices:\n",
        "    trainx_low.append(train_x[i])\n",
        "    trainy_low.append(train_y[i])\n",
        "for i in high_indices:\n",
        "    trainx_high.append(train_x[i])\n",
        "    trainy_high.append(train_y[i])\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbP7pQFe3Pra"
      },
      "source": [
        "# **Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s6Dvvp4-0ik",
        "outputId": "c27f43ac-34a8-4deb-9354-bb527a0e4933"
      },
      "source": [
        "from models_celeb.preact_resnet import PreActResNet18\n",
        "net=PreActResNet18()\n",
        "model_dict = torch.load('/content/drive/My Drive/genderclassifier_new_3.pth')\n",
        "net.load_state_dict(model_dict['net'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHtTR5K93vMn"
      },
      "source": [
        "# **GradCAM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7kK0AkzAOSX"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "        \n",
        "        # define the PreActResNet18\n",
        "        self.resnet = net\n",
        "        \n",
        "        # isolate the feature blocks\n",
        "        self.features = nn.Sequential(self.resnet.conv1,\n",
        "                                      self.resnet.layer1, \n",
        "                                      self.resnet.layer2, \n",
        "                                      self.resnet.layer3, \n",
        "                                      self.resnet.layer4)\n",
        "        \n",
        "       \n",
        "        \n",
        "        # classifier\n",
        "        self.classifier = self.resnet.linear\n",
        "        \n",
        "        # gradient placeholder\n",
        "        self.gradient = None\n",
        "    \n",
        "    # hook for the gradients\n",
        "    def activations_hook(self, grad):\n",
        "        self.gradient = grad\n",
        "    \n",
        "    def get_gradient(self):\n",
        "        return self.gradient\n",
        "    \n",
        "    def get_activations(self, x):\n",
        "        return self.features(x)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # extract the features\n",
        "        x = self.features(x)\n",
        "        \n",
        "        # register the hook\n",
        "        h = x.register_hook(self.activations_hook)\n",
        "        \n",
        "        x=F.avg_pool2d(x, 4)\n",
        "        # complete the forward pass\n",
        "        x = x.view((x.size(0), -1))\n",
        "        x = self.classifier(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugBKB2TvAkmS"
      },
      "source": [
        "resnet=ResNet()\n",
        "_=resnet.eval()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_LI8Hxn61UK"
      },
      "source": [
        "# **store saliency maps of low valued points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9YMSyvMEy5n"
      },
      "source": [
        "low_correct_indices=[]\n",
        "low_correct_y=[]\n",
        "low_wrong_idices=[]\n",
        "low_wrong_pred=[]\n",
        "low_wrong_actual=[]\n",
        "for i in range(len(low_indices)):\n",
        "    inp=trainx_low[i]\n",
        "    inp=inp.float().reshape(1,3,32,32)\n",
        "    pred=resnet(inp)\n",
        "    index=pred.argmax(dim=1)\n",
        "    pred[:,index].backward()\n",
        "    gradients = resnet.gradient\n",
        "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
        "    activations = resnet.get_activations(inp).detach()\n",
        "    for j in range(512):\n",
        "        activations[:, j, :, :] *= pooled_gradients[j]\n",
        "    heatmap = torch.mean(activations, dim=1).squeeze()\n",
        "    \n",
        "    heatmap = np.maximum(heatmap.cpu(), 0)\n",
        "    heatmap /= torch.max(heatmap)\n",
        "    #plt.matshow(heatmap.squeeze())\n",
        "    heatmap = heatmap.numpy()\n",
        "    inp=inv_normalize(inp)\n",
        "    inp=inp.cpu().reshape(3,32,32)\n",
        "    inp=inp.numpy().transpose(1,2,0)\n",
        "    inp=np.uint8(255*inp)\n",
        "    inp=cv2.resize(inp,(224,224))\n",
        "    print(i,\" predicted:\",encoder[int(index)],\" actual:\",encoder[trainy_low[i]])\n",
        "    plt.imshow(inp)\n",
        "    plt.show()\n",
        "    heatmap = cv2.resize(heatmap, (inp.shape[1], inp.shape[0]))\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    if index==trainy_low[i]:\n",
        "        f2='./low_hmap_correct'+str(i)+'.jpg'\n",
        "        file_name='./low_superimposed_correct'+str(i)+'.jpg'\n",
        "    else:\n",
        "        f2='./low_hmap_wrong'+str(i)+'.jpg'\n",
        "        file_name='./low_superimposed_wrong'+str(i)+'.jpg'\n",
        "    cv2.imwrite(f2,heatmap)\n",
        "    superimposed_img = heatmap*0.4  + inp\n",
        "    si2=heatmap*inp\n",
        "    #plt.imshow(superimposed_img)\n",
        "    cv2.imwrite(file_name, superimposed_img)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvvQUSt78k9B"
      },
      "source": [
        "# **store saliency maps of high valued points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlWx1xI4TZZX"
      },
      "source": [
        "\n",
        "for i in range(80):\n",
        "    inp=trainx_high[i]\n",
        "    inp=inp.float().reshape(1,3,32,32)\n",
        "    pred=resnet(inp)\n",
        "    index=pred.argmax(dim=1)\n",
        "    pred[:,index].backward()\n",
        "    gradients = resnet.gradient\n",
        "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
        "    activations = resnet.get_activations(inp).detach()\n",
        "    for j in range(512):\n",
        "        activations[:, j, :, :] *= pooled_gradients[j]\n",
        "    heatmap = torch.mean(activations, dim=1).squeeze()\n",
        "    \n",
        "    heatmap = np.maximum(heatmap.cpu(), 0)\n",
        "    heatmap /= torch.max(heatmap)\n",
        "    #plt.matshow(heatmap.squeeze())\n",
        "    heatmap = heatmap.numpy()\n",
        "    inp=inv_normalize(inp)\n",
        "    inp=inp.cpu().reshape(3,32,32)\n",
        "    inp=inp.numpy().transpose(1,2,0)\n",
        "    inp=np.uint8(255*inp)\n",
        "    inp=cv2.resize(inp,(224,224))\n",
        "    print(i,\" predicted:\",encoder[int(index)],\" actual:\",encoder[trainy_high[i]])\n",
        "    plt.imshow(inp)\n",
        "    plt.show()\n",
        "    heatmap = cv2.resize(heatmap, (inp.shape[1], inp.shape[0]))\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    if index==trainy_high[i]:\n",
        "        f2='./high_hmap_correct'+str(i)+'.jpg'\n",
        "        file_name='./high_superimposed_correct'+str(i)+'.jpg'\n",
        "    else:\n",
        "        f2='./high_hmap_wrong'+str(i)+'.jpg'\n",
        "        file_name='./high_superimposed_wrong'+str(i)+'.jpg'\n",
        "    cv2.imwrite(f2,heatmap)\n",
        "    superimposed_img = heatmap*0.4  + inp\n",
        "    cv2.imwrite(file_name, superimposed_img)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}